{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#COVID-19 & INFECTION DETECTION WITH CT-SCAN IMAGES\n",
        "##- Binary Classification & CNN based Model -\n",
        "\n",
        "Group Members:\n",
        "\n",
        "Muneeza Iftikhar (02-136212-012)\n",
        "\n",
        "Hafsa Hafeez Siddiqui (02-136212-026)\n",
        "\n",
        "Aqsa Khan (02-136212-039)"
      ],
      "metadata": {
        "id": "YJxnNtwRpXRY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYuHzA8-8ee8"
      },
      "source": [
        "# Data Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JQeEnV-K83Kg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def define_paths(data_dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "\n",
        "    folds = os.listdir(data_dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "\n",
        "    return filepaths, labels\n",
        "\n",
        "\n",
        "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
        "def define_df(files, classes):\n",
        "    Fseries = pd.Series(files, name= 'filepaths')\n",
        "    Lseries = pd.Series(classes, name='labels')\n",
        "    return pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "\n",
        "# Split dataframe into train, valid, and test\n",
        "def split_data(data_dir):\n",
        "    # create train dataframe\n",
        "    files, classes = define_paths(data_dir)\n",
        "    df = define_df(files, classes)\n",
        "\n",
        "    strat = df['labels']\n",
        "    # split the whole dataset into train and non-train dataframes\n",
        "    train_df, dummy_df = train_test_split(df, train_size=0.7, shuffle=True, random_state=101, stratify=strat)\n",
        "\n",
        "    # from the non-train dataset, create validation and test dataframes\n",
        "    strat = dummy_df['labels']\n",
        "    validation_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=101, stratify=strat)\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "\n",
        "def data_generators(data_dir, img_size, batch_size, class_mode, color_mode):\n",
        "\n",
        "    from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "    train_df, val_df, test_df = split_data(data_dir)\n",
        "\n",
        "    # model input is taken ONLY from imagedatagenerator\n",
        "    # initializing the imagedatagenerator class\n",
        "    training_data_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    testing_data_generator = ImageDataGenerator(\n",
        "        rescale=1./255      # normalisation of images between 0 and 1 from 0 to 255 pixels\n",
        "    )\n",
        "\n",
        "    # initialize training, validation and testing generators\n",
        "    train_generator = training_data_generator.flow_from_dataframe(\n",
        "        train_df,\n",
        "        x_col='filepaths',\n",
        "        y_col='labels',\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode,\n",
        "        color_mode=color_mode,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_generator = testing_data_generator.flow_from_dataframe(\n",
        "        val_df,\n",
        "        x_col='filepaths',\n",
        "        y_col='labels',\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode,\n",
        "        color_mode=color_mode,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    test_generator = testing_data_generator.flow_from_dataframe(\n",
        "        test_df,\n",
        "        x_col='filepaths',\n",
        "        y_col='labels',\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode,\n",
        "        color_mode=color_mode,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator, test_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAg_rQIg8-TR"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VkidkxnV7EU5"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# defining Convolutional Neural Network (CNN)\n",
        "# after initializing the imagedatagenerator\n",
        "\n",
        "class Classifier:\n",
        "    def get_model(input_shape):\n",
        "        model = Sequential() # runs the layers in sequence\n",
        "        model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "        model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(0.25)) # used to reduce overfitting\n",
        "\n",
        "        model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def VGG_model(input_shape):         # VERY LARGE CNN MODEL ON WHICH WE DID TRANSFER LEARNING\n",
        "                                                              # pre-trained model\n",
        "        from keras.applications import VGG16\n",
        "\n",
        "        num_classes=2\n",
        "\n",
        "        # VGG_model HAS BEEN TRAINED ON imagenet DATASET\n",
        "        vgg = VGG16(input_shape=input_shape, weights = 'imagenet', include_top = False)\n",
        "\n",
        "        for layer in vgg.layers:    # only using feature extractor, every layer will remain the same\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = Flatten()(vgg.output)   # using our classification layer\n",
        "        x = Dense(128, activation = 'relu')(x)\n",
        "        x = Dense(64, activation = 'relu')(x)\n",
        "        x = Dense(num_classes, activation = 'softmax')(x)\n",
        "\n",
        "        model = Model(inputs = vgg.input, outputs = x)\n",
        "\n",
        "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIeL-G7E9VSL"
      },
      "source": [
        "# Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU5rQ6Kl9bhD",
        "outputId": "190eab5a-ab0a-487c-82a0-80a52f51c2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Found 1736 validated image filenames belonging to 2 classes.\n",
            "Found 372 validated image filenames belonging to 2 classes.\n",
            "Found 373 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.6905 - accuracy: 0.6797\n",
            "Epoch 1: val_loss improved from inf to 0.60027, saving model to /content/drive/MyDrive/models/vgg_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 262s 5s/step - loss: 0.6905 - accuracy: 0.6797 - val_loss: 0.6003 - val_accuracy: 0.6774\n",
            "Epoch 2/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.7788\n",
            "Epoch 2: val_loss improved from 0.60027 to 0.41059, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 37s 675ms/step - loss: 0.4621 - accuracy: 0.7788 - val_loss: 0.4106 - val_accuracy: 0.8011\n",
            "Epoch 3/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3945 - accuracy: 0.8209\n",
            "Epoch 3: val_loss improved from 0.41059 to 0.33458, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 37s 673ms/step - loss: 0.3945 - accuracy: 0.8209 - val_loss: 0.3346 - val_accuracy: 0.8602\n",
            "Epoch 4/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.8410\n",
            "Epoch 4: val_loss did not improve from 0.33458\n",
            "55/55 [==============================] - 35s 640ms/step - loss: 0.3625 - accuracy: 0.8410 - val_loss: 0.3938 - val_accuracy: 0.8172\n",
            "Epoch 5/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8364\n",
            "Epoch 5: val_loss did not improve from 0.33458\n",
            "55/55 [==============================] - 36s 657ms/step - loss: 0.3806 - accuracy: 0.8364 - val_loss: 0.3565 - val_accuracy: 0.8360\n",
            "Epoch 6/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8548\n",
            "Epoch 6: val_loss did not improve from 0.33458\n",
            "55/55 [==============================] - 35s 635ms/step - loss: 0.3506 - accuracy: 0.8548 - val_loss: 0.3687 - val_accuracy: 0.8441\n",
            "Epoch 7/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8514\n",
            "Epoch 7: val_loss improved from 0.33458 to 0.28484, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 37s 677ms/step - loss: 0.3299 - accuracy: 0.8514 - val_loss: 0.2848 - val_accuracy: 0.8763\n",
            "Epoch 8/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8295\n",
            "Epoch 8: val_loss did not improve from 0.28484\n",
            "55/55 [==============================] - 38s 694ms/step - loss: 0.3872 - accuracy: 0.8295 - val_loss: 0.2922 - val_accuracy: 0.8737\n",
            "Epoch 9/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8664\n",
            "Epoch 9: val_loss improved from 0.28484 to 0.26395, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 36s 653ms/step - loss: 0.3127 - accuracy: 0.8664 - val_loss: 0.2640 - val_accuracy: 0.8952\n",
            "Epoch 10/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.8456\n",
            "Epoch 10: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 35s 638ms/step - loss: 0.3471 - accuracy: 0.8456 - val_loss: 0.2679 - val_accuracy: 0.8871\n",
            "Epoch 11/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.8474\n",
            "Epoch 11: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 35s 634ms/step - loss: 0.3346 - accuracy: 0.8474 - val_loss: 0.2879 - val_accuracy: 0.8871\n",
            "Epoch 12/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8641\n",
            "Epoch 12: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 35s 642ms/step - loss: 0.3200 - accuracy: 0.8641 - val_loss: 0.3181 - val_accuracy: 0.8871\n",
            "Epoch 13/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.8577\n",
            "Epoch 13: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 36s 648ms/step - loss: 0.3374 - accuracy: 0.8577 - val_loss: 0.2831 - val_accuracy: 0.8844\n",
            "Epoch 14/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8773\n",
            "Epoch 14: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 35s 632ms/step - loss: 0.3149 - accuracy: 0.8773 - val_loss: 0.2954 - val_accuracy: 0.8737\n",
            "Epoch 15/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.8733\n",
            "Epoch 15: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 38s 697ms/step - loss: 0.2998 - accuracy: 0.8733 - val_loss: 0.3782 - val_accuracy: 0.8226\n",
            "Epoch 16/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.8422\n",
            "Epoch 16: val_loss did not improve from 0.26395\n",
            "55/55 [==============================] - 35s 641ms/step - loss: 0.3452 - accuracy: 0.8422 - val_loss: 0.3470 - val_accuracy: 0.8522\n",
            "Epoch 17/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.8646\n",
            "Epoch 17: val_loss improved from 0.26395 to 0.25257, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 39s 702ms/step - loss: 0.3025 - accuracy: 0.8646 - val_loss: 0.2526 - val_accuracy: 0.9032\n",
            "Epoch 18/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2987 - accuracy: 0.8704\n",
            "Epoch 18: val_loss did not improve from 0.25257\n",
            "55/55 [==============================] - 35s 642ms/step - loss: 0.2987 - accuracy: 0.8704 - val_loss: 0.2552 - val_accuracy: 0.9059\n",
            "Epoch 19/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.8865\n",
            "Epoch 19: val_loss did not improve from 0.25257\n",
            "55/55 [==============================] - 35s 633ms/step - loss: 0.2771 - accuracy: 0.8865 - val_loss: 0.3044 - val_accuracy: 0.8683\n",
            "Epoch 20/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8767\n",
            "Epoch 20: val_loss improved from 0.25257 to 0.24103, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 39s 705ms/step - loss: 0.2890 - accuracy: 0.8767 - val_loss: 0.2410 - val_accuracy: 0.9059\n",
            "Epoch 21/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2644 - accuracy: 0.8796\n",
            "Epoch 21: val_loss did not improve from 0.24103\n",
            "55/55 [==============================] - 36s 657ms/step - loss: 0.2644 - accuracy: 0.8796 - val_loss: 0.3576 - val_accuracy: 0.8360\n",
            "Epoch 22/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.8785\n",
            "Epoch 22: val_loss did not improve from 0.24103\n",
            "55/55 [==============================] - 37s 673ms/step - loss: 0.2733 - accuracy: 0.8785 - val_loss: 0.3460 - val_accuracy: 0.8253\n",
            "Epoch 23/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2590 - accuracy: 0.8923\n",
            "Epoch 23: val_loss did not improve from 0.24103\n",
            "55/55 [==============================] - 35s 640ms/step - loss: 0.2590 - accuracy: 0.8923 - val_loss: 0.2881 - val_accuracy: 0.8683\n",
            "Epoch 24/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.8594\n",
            "Epoch 24: val_loss improved from 0.24103 to 0.22428, saving model to /content/drive/MyDrive/models/vgg_model.h5\n",
            "55/55 [==============================] - 39s 707ms/step - loss: 0.3208 - accuracy: 0.8594 - val_loss: 0.2243 - val_accuracy: 0.9220\n",
            "Epoch 25/25\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.2606 - accuracy: 0.8871\n",
            "Epoch 25: val_loss did not improve from 0.22428\n",
            "55/55 [==============================] - 35s 642ms/step - loss: 0.2606 - accuracy: 0.8871 - val_loss: 0.2669 - val_accuracy: 0.8763\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# setting only relative path to get dataset, not actually getting the dataset\n",
        "data_dir = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "# setting only relative path to save / retrieve models\n",
        "# (because model will not be trained everytime you want to use it)\n",
        "model_dir = '/content/drive/MyDrive/models'\n",
        "\n",
        "\n",
        "# configuration parameters for the classifier we created:\n",
        "# INPUT_SHAPE = (128,128,1)                    # input shape of model\n",
        "# model = Classifier.get_model(INPUT_SHAPE)\n",
        "# COLOR_MODE = 'greyscale'                     # read the images as greyscale - 1 channel\n",
        "# CLASS_MODE = 'categorical'                   # softmax layer at the output of the model\n",
        "# model_path = '/model.h5'                            # this path will be used to save the model\n",
        "# MODEL_NAME = 'CLASSY'\n",
        "\n",
        "\n",
        "# configuration parameters for the VGG model (used for transfer learning):\n",
        "INPUT_SHAPE = (224,224,3)                       # input shape of model\n",
        "model = Classifier.VGG_model(INPUT_SHAPE)\n",
        "COLOR_MODE = 'rgb'                              # read the images as colored - 3 channels because VGG model is trained on colored images even though our dataset images are greyscale\n",
        "CLASS_MODE = 'categorical'                      # softmax layer at the output of the model\n",
        "model_path = '/vgg_model.h5'                    # this path will be used to save the model\n",
        "MODEL_NAME = 'VGG'\n",
        "\n",
        "\n",
        "# input_shape of model (entry layer) and image size should be the same\n",
        "# INPUT_SHAPE[0] = 224 , INPUT_SHAPE[1] = 224\n",
        "img_size = (INPUT_SHAPE[0],INPUT_SHAPE[1])\n",
        "\n",
        "# approximated by dividing the number of images in the training set for a single iteration\n",
        "# when all the batches are completed running that is equal to total number of images which is equal to 1 epoch\n",
        "batch_size = 32\n",
        "\n",
        "# get ImageDataGenerators.flow_from_dataframe - get training, validation, and testing data generators\n",
        "train_generator, validation_generator, test_generator = \\\n",
        "    data_generators(data_dir=data_dir, img_size=img_size, batch_size=batch_size, class_mode=CLASS_MODE, color_mode=COLOR_MODE)\n",
        "\n",
        "# defining model\n",
        "\n",
        "# Callback functions for monitoring, training, and saving purposes of the fitting of the model:\n",
        "# checkpoint and early_stopping functions w.r.t val_loss by default\n",
        "\n",
        "# saves the model if the validation loss is lowest\n",
        "# verbose: Mode 0 is silent, and mode 1 displays messages when the callback takes an action\n",
        "checkpoint = ModelCheckpoint(model_dir+model_path, verbose=1, save_best_only=True)\n",
        "\n",
        "# stops the training if the validation loss does not decrease constantly\n",
        "# patience: Number of epochs with no improvement after which training will be stopped.\n",
        "early_stopping = EarlyStopping( patience=8)\n",
        "\n",
        "\n",
        "##################################### TRAIN THE MODEL #####################################\n",
        "# train the model using fit_generator\n",
        "# after defining the CNN model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=len(validation_generator),\n",
        "    callbacks=[checkpoint, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Cf8dimEoya"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwLYy4xBEtV7",
        "outputId": "f25911f4-7d4b-4d5e-b15d-9750dc91e2ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 372 validated image filenames belonging to 2 classes.\n",
            "12/12 [==============================] - 3s 199ms/step - loss: 0.2243 - accuracy: 0.9220\n",
            "\n",
            "\n",
            "\n",
            "Saved VGG Model, accuracy: 92.20%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/dataset'\n",
        "model_dir = '/content/drive/MyDrive/models/vgg_model.h5'\n",
        "\n",
        "_, test_df, _ = split_data(data_dir)\n",
        "\n",
        "\n",
        "testing_data_generator = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "    )\n",
        "\n",
        "test_generator = testing_data_generator.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='filepaths',\n",
        "    y_col='labels',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    color_mode='rgb',\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "model = keras.models.load_model(model_dir)\n",
        "\n",
        "loss, acc = model.evaluate(test_generator)\n",
        "\n",
        "print('\\n\\n')\n",
        "print('Saved VGG Model, accuracy: {:5.2f}%'.format(100*acc))\n",
        "print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}